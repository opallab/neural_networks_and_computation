# Computational Capability and Efficiency of Neural Networks: A Repository of Papers

Contributed by Kimon Fountoulakis

## [Content](#content)

<table>
<tr><td colspan="1"><a href="#simulation-results">1. Simulation Results</a></td></tr> 
<tr>
    <td>&ensp;<a href="#recurrent-neural-networks">1.1 Recurrent neural networks</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#transformers-simulation">1.2 Transformers</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#feedforward-simulation">1.3 Feedforward neural networks</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#graph-neural-networks">1.4 Graph neural networks</a></td>
</tr>
<tr><td colspan="1"><a href="#learning-results">2. Learning Results</a></td></tr> 
<tr>
    <td>&ensp;<a href="#transformers-learning">2.1 Transformers</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#feedforward-learning">2.2 Feedforward neural networks</a></td>
</tr>
<tr><td colspan="1"><a href="#empirical">3. Empirical</a></td></tr> 
<tr><td colspan="1"><a href="#formal-languages">4. Formal Languages</a></td></tr> 
</table>
    
## Simulation Results

### Recurrent neural networks

1. **Supervised Neural Networks for the Classification of Structures.** Journal of Computer and System Sciences 1995. [paper](https://www.sciencedirect.com/science/article/pii/S0022000085710136)

    *H.T. Siegelmann, E.D. Sontag*

<a name="transformers-simulation"></a>
### Transformers
    
1. **Attention is Turing Complete.** Journal of Machine Learning Research 2021. [paper](https://jmlr.org/papers/v22/20-302.html)

    *Jorge Pérez, Pablo Barceló, Javier Marinkovic*

1. **Looped Transformers as Programmable Computers.** ICML 2023. [paper](https://proceedings.mlr.press/v202/giannou23a.html)

    *Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos*

1. **Exposing Attention Glitches with Flip-Flop Language Modeling.** NeurIPS 2023. [paper](https://openreview.net/forum?id=VzmpXQAn6E)

    *Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang*

1. **Transformers Learn Shortcuts to Automata.** ICLR 2023. [paper](https://openreview.net/forum?id=De4FYqjFueZ)

    *Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang*

1. **Memory Augmented Large Language Models are Computationally Universal.** arXiv 2023. [paper](https://arxiv.org/abs/2301.04589)

    *Dale Schuurmans*

1. **Chain of Thought Empowers Transformers to Solve Inherently Serial Problems.** ICLR 2024. [paper](https://openreview.net/forum?id=3EWTEy9MTM)

    *Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma*

1. **Representational Capabilities of Feed-Forward and Sequential Neural Architectures.** PhD Thesis 2024. [paper](https://www.proquest.com/openview/0a8f8efe0d4041ae5e6bab292a2d19a1/1?pq-origsite=gscholar&cbl=18750&diss=y)

    *Sanford, Clayton Hendrick*

1. **Transformers, parallel computation, and logarithmic depth.** ICML 2024. [paper](https://dl.acm.org/doi/10.5555/3692070.3693833)

    *Clayton Sanford, Daniel Hsu, Matus Telgarsky*

1. **Understanding Transformer Reasoning Capabilities via Graph Algorithms.** NeurIPS 2024. [paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/8f395480c04ac6dfb2c2326a639df88e-Abstract-Conference.html)

    *Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, Vahab Mirrokni*

1. **A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers.** NeurIPS 2024 Workshop M3L. [paper](https://openreview.net/forum?id=njycONK0JG)

    *William Merrill, Ashish Sabharwal*

1. **On Limitations of the Transformer Architecture.** COLM 2024. [paper](https://openreview.net/forum?id=KidynPuLNW#discussion)

    *Binghui Peng, Srini Narayanan, Christos Papadimitriou*

1. **Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers.** arXiv 2025. [paper](https://arxiv.org/abs/2503.01805)

    *Gilad Yehudai, Clayton Sanford, Maya Bechler-Speicher, Orr Fischer, Ran Gilad-Bachrach, Amir Globerson*

1. **Positional Attention: Expressivity and Learnability of Algorithmic Computation.** arXiv 2025. [paper](https://arxiv.org/abs/2410.01686)

    *Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis*

1. **Round and Round We Go! What makes Rotary Positional Encodings useful?.** ICLR 2025. [paper](https://openreview.net/forum?id=GtvuNrk58a)

    *Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković*

1. **Reasoning with Latent Thoughts: On the Power of Looped Transformers.** ICLR 2025. [paper](https://openreview.net/forum?id=din0lGfZFd)

    *Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi*

<a name="feedforward-simulation"></a>
### Feedforward neural networks

1. **Provably good solutions to the knapsack problem via neural networks
of bounded size.** AAAI 2021. [paper](https://cdn.aaai.org/ojs/16939/16939-13-20433-1-2-20210518.pdf)

    *Christoph Hertrich, Martin Skutella*

1. **ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation.** Integer Programming and Combinatorial Optimization 2023. [paper](https://link.springer.com/chapter/10.1007/978-3-031-32726-1_14)

    *Christoph Hertrich, Leon Sering*

1. **Representational Capabilities of Feed-Forward and Sequential Neural Architectures.** PhD Thesis 2024. [paper](https://www.proquest.com/openview/0a8f8efe0d4041ae5e6bab292a2d19a1/1?pq-origsite=gscholar&cbl=18750&diss=y)

    *Sanford, Clayton Hendrick*

### Graph neural networks

1. **Graph neural networks extrapolate out-of-distribution for shortest paths.** arXiv 2025. [paper](https://arxiv.org/abs/2503.19173)

    *Robert R. Nerem, Samantha Chen, Sanjoy Dasgupta, Yusu Wang*

1. **What graph neural networks cannot learn: depth vs width.** ICLR 2020. [paper](https://openreview.net/forum?id=B1l2bp4YwS)

    *Andreas Loukas*

1. **Simulation of Graph Algorithms with Looped Transformers.** ICML 2024. [paper]([https://openreview.net/forum?id=B1l2bp4YwS](https://proceedings.mlr.press/v235/back-de-luca24a.html))

    *Artur Back De Luca, Kimon Fountoulakis*

1. **Graph Transformers Dream of Electric Flow.** ICLR 2025. [paper](https://openreview.net/forum?id=rWQDzq3O5c&noteId=VeksyxMusa)

    *Xiang Cheng, Lawrence Carin, Suvrit Sra*

1. **Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems.** AISTATS 2024. [paper](https://openreview.net/forum?id=rWQDzq3O5c&noteId=VeksyxMusa)

    *Chendi Qian, Didier Chételat, Christopher Morris*

## Learning Results  

<a name="transformers-learning"></a>
### Transformers

1. **Positional Attention: Expressivity and Learnability of Algorithmic Computation.** arXiv 2025. [paper](https://arxiv.org/abs/2410.01686)

    *Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis*

<a name="feedforward-learning"></a>
### Feedforward neural networks

1. **Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks.** arXiv 2025. [paper](https://arxiv.org/abs/2502.16763)

    *George Giapitzakis, Artur Back de Luca, Kimon Fountoulakis*

## Empirical

1. **Learning to Execute.** arXiv 2015. [paper](https://arxiv.org/abs/1410.4615)

    *Wojciech Zaremba, Ilya Sutskever*

1. **Neural Programmer-Interpreters.** arXiv 2015. [paper](https://arxiv.org/abs/1511.06279)

    *Scott Reed, Nando de Freitas*

1. **Neural Programmer: Inducing Latent Programs with Gradient Descent.** arXiv 2016. [paper](https://arxiv.org/abs/1511.04834)

    *Arvind Neelakantan, Quoc V. Le, Ilya Sutskever*

1. **Deep Neural Solver for Math Word Problems.** arXiv 2017. [paper](https://aclanthology.org/D17-1088/)

    *Yan Wang, Xiaojiang Liu, and Shuming Shi*

1. **Analysing Mathematical Reasoning Abilities of Neural Models.** arXiv 2019. [paper](https://arxiv.org/abs/1904.01557)

    *David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli*

1. **Investigating the Limitations of Transformers with Simple Arithmetic Tasks.** arXiv 2021. [paper](https://arxiv.org/abs/2102.13019)

    *Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin*

1. **A Primer for Neural Arithmetic Logic Modules.** arXiv 2022. [paper](https://arxiv.org/abs/2101.09530)

    *Bhumika Mistry, Katayoun Farrahi, Jonathon Hare*

1. **Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.** arXiv 2022. [paper](https://arxiv.org/abs/2201.02177)

    *Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra*

1. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.** arXiv 2023. [paper](https://arxiv.org/abs/2201.11903)

    *Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou*

1. **Implicit Chain of Thought Reasoning via Knowledge Distillation.** arXiv 2023. [paper](https://arxiv.org/abs/2311.01460)

    *Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber*

1. **Positional Description Matters for Transformers Arithmetic.** arXiv 2023. [paper](https://arxiv.org/abs/2311.14737)

    *Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang*

1. **Length Generalization in Arithmetic Transformers.** arXiv 2023. [paper](https://arxiv.org/abs/2306.15400)

    *Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, François Charton*

1. **Transformers Can Do Arithmetic with the Right Embeddings.** arXiv 2024. [paper](https://arxiv.org/abs/2405.17399)

    *Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, Tom Goldstein*

1. **From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.** arXiv 2024. [paper](https://arxiv.org/abs/2405.14838)

    *Yuntian Deng, Yejin Choi, Stuart Shieber*

## Formal Languages  

1. **Neural Networks and the Chomsky Hierarchy.** ICLR 2023. [paper](https://openreview.net/forum?id=WbxHAzkeQcn)

    *Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, Pedro A. Ortega*

1. **Training Neural Networks as Recognizers of Formal Languages.** ICLR 2025. [paper](https://openreview.net/forum?id=aWLQTbfFgV)

    *Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell*
