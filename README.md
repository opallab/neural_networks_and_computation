# Computational Capability and Efficiency of Neural Networks: A Repository of Papers

Contributed by Kimon Fountoulakis

## [Content](#content)

<table>
<tr><td colspan="1"><a href="#simulation-results">1. Simulation Results</a></td></tr> 
<tr>
    <td>&ensp;<a href="#recurrent-neural-networks">1.1 Recurrent neural networks</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#transformers-simulation">1.2 Transformers</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#feedforward-simulation">1.3 Feedforward neural networks</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#graph-neural-networks">1.4 Graph neural networks</a></td>
</tr>
<tr><td colspan="1"><a href="#learning-results">2. Learning Results</a></td></tr> 
<tr>
    <td>&ensp;<a href="#transformers-learning">2.1 Transformers</a></td>
</tr>
<tr>
    <td>&ensp;<a href="#feedforward-learning">2.2 Feedforward neural networks</a></td>
</tr>
</table>
    
## Simulation Results

### Recurrent neural networks

1. **Supervised Neural Networks for the Classification of Structures.** Journal of Computer and System Sciences 1995. [paper](https://www.sciencedirect.com/science/article/pii/S0022000085710136)

    *H.T. Siegelmann, E.D. Sontag*

<a name="transformers-simulation"></a>
### Transformers
    
1. **Attention is Turing Complete.** Journal of Machine Learning Research 2021. [paper](https://jmlr.org/papers/v22/20-302.html)

    *Jorge Pérez, Pablo Barceló, Javier Marinkovic*

1. **Looped Transformers as Programmable Computers.** ICML 2023. [paper](https://proceedings.mlr.press/v202/giannou23a.html)

    *Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos*

1. **Exposing Attention Glitches with Flip-Flop Language Modeling.** NeurIPS 2023. [paper](https://openreview.net/forum?id=VzmpXQAn6E)

    *Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang*

1. **Transformers Learn Shortcuts to Automata .** ICLR 2023. [paper](https://openreview.net/forum?id=De4FYqjFueZ)

    *Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang*

1. **Representational Capabilities of Feed-Forward and Sequential Neural Architectures.** PhD Thesis 2024. [paper](https://www.proquest.com/openview/0a8f8efe0d4041ae5e6bab292a2d19a1/1?pq-origsite=gscholar&cbl=18750&diss=y)

    *Sanford, Clayton Hendrick*

1. **Transformers, parallel computation, and logarithmic depth.** ICML 2024. [paper](https://dl.acm.org/doi/10.5555/3692070.3693833)

    *Clayton Sanford, Daniel Hsu, Matus Telgarsky*

1. **Understanding Transformer Reasoning Capabilities via Graph Algorithms.** NeurIPS 2024. [paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/8f395480c04ac6dfb2c2326a639df88e-Abstract-Conference.html)

    *Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, Vahab Mirrokni*

1. **A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers.** NeurIPS 2024 Workshop M3L. [paper](https://openreview.net/forum?id=njycONK0JG)

    *William Merrill, Ashish Sabharwal*

1. **On Limitations of the Transformer Architecture.** COLM 2024. [paper](https://openreview.net/forum?id=KidynPuLNW#discussion)

    *Binghui Peng, Srini Narayanan, Christos Papadimitriou*

1. **Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers.** arXiv 2025. [paper](https://arxiv.org/abs/2503.01805)

    *Gilad Yehudai, Clayton Sanford, Maya Bechler-Speicher, Orr Fischer, Ran Gilad-Bachrach, Amir Globerson*

1. **Positional Attention: Expressivity and Learnability of Algorithmic Computation.** arXiv 2025. [paper](https://arxiv.org/abs/2410.01686)

    *Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis*

<a name="feedforward-simulation"></a>
### Feedforward neural networks

1. **Provably good solutions to the knapsack problem via neural networks
of bounded size.** AAAI 2021. [paper](https://cdn.aaai.org/ojs/16939/16939-13-20433-1-2-20210518.pdf)

    *Christoph Hertrich, Martin Skutella*

1. **ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation.** Integer Programming and Combinatorial Optimization 2023. [paper](https://link.springer.com/chapter/10.1007/978-3-031-32726-1_14)

    *Christoph Hertrich, Leon Sering*

1. **Representational Capabilities of Feed-Forward and Sequential Neural Architectures.** PhD Thesis 2024. [paper](https://www.proquest.com/openview/0a8f8efe0d4041ae5e6bab292a2d19a1/1?pq-origsite=gscholar&cbl=18750&diss=y)

    *Sanford, Clayton Hendrick*

### Graph neural networks

1. **Graph neural networks extrapolate out-of-distribution for shortest paths.** arXiv 2025. [paper](https://arxiv.org/abs/2503.19173)

    *Robert R. Nerem, Samantha Chen, Sanjoy Dasgupta, Yusu Wang*

1. **What graph neural networks cannot learn: depth vs width.** ICLR 2020. [paper](https://openreview.net/forum?id=B1l2bp4YwS)

    *Andreas Loukas*

1. **Simulation of Graph Algorithms with Looped Transformers.** ICML 2024. [paper]([https://openreview.net/forum?id=B1l2bp4YwS](https://proceedings.mlr.press/v235/back-de-luca24a.html))

    *Artur Back De Luca, Kimon Fountoulakis*

1. **Graph Transformers Dream of Electric Flow.** ICLR 2025. [paper](https://openreview.net/forum?id=rWQDzq3O5c&noteId=VeksyxMusa)

    *Xiang Cheng, Lawrence Carin, Suvrit Sra*

## Learning Results  

<a name="transformers-learning"></a>
### Transformers

1. **Positional Attention: Expressivity and Learnability of Algorithmic Computation.** arXiv 2025. [paper](https://arxiv.org/abs/2410.01686)

    *Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis*

<a name="feedforward-learning"></a>
### Feedforward neural networks

1. **Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity.** arXiv 2025. [paper](https://arxiv.org/abs/2502.16763)

    *George Giapitzakis, Artur Back de Luca, Kimon Fountoulakis*
